{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielVeloso077/AutoMLAgents/blob/main/LangChain_Prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<table style=\"margin: auto; background-color: white;\">\n",
        "    <tr>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1S3xpVbkzpBAG51PyuRyIioecvZiHGSap' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1S3xpVbkzpBAG51PyuRyIioecvZiHGSap' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1S3xpVbkzpBAG51PyuRyIioecvZiHGSap' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "    </tr>\n",
        "</table>\n",
        "<font face=\"Verdana\" size=1 color='#707bf8' font-family= 'Alike Angular'> *Este material foi desenvolvido como parte do Projeto de Pesquisa em Parceria com a KUNUMI S/A <font>\n"
      ],
      "metadata": {
        "id": "c-1E9xStNhZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"Rage\" size=7 color='#707bf8'> LangChain: Prompting <font>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6EPTm1oHUAsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Instala√ß√£o LangChain"
      ],
      "metadata": {
        "id": "B0_E0uDZp6S_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3NwJxuco_7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d42fb8-f960-4357-cdc3-d7e6378c4a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.68)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.68)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.4)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acesso √† plataforma DeepInfra"
      ],
      "metadata": {
        "id": "fYMjBzL-hzm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"DEEPINFRA_API_TOKEN\"] = getpass(\"Digite sua chave da DeepInfra: \")"
      ],
      "metadata": {
        "id": "7SjFdoEArAAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ce03e4-66d2-44b7-d02e-bbd5b1f97a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Digite sua chave da DeepInfra: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí∏ Modelos de Chat da DeepInfra (ordenados por pre√ßo de entrada)\n",
        "\n",
        "Link para p√°gina de modelos: [https://deepinfra.com/models/text-generation](https://deepinfra.com/models/text-generation)\n",
        "\n",
        "| Modelo                                             | Contexto | Pre√ßo por 1M tokens (entrada / sa√≠da) |\n",
        "|----------------------------------------------------|:--------:|:-------------------------------------:|\n",
        "| deepseek-ai/DeepSeek-R1                            | 160k     | 0.45 / 2.15                           |\n",
        "| deepseek-ai/DeepSeek-R1-0528                       | 160k     | 0.50 / 2.15                           |\n",
        "| deepseek-ai/DeepSeek-R1-Turbo                      | 32k      | 1.00 / 3.00                           |\n",
        "| deepseek-ai/DeepSeek-V3-0324                       | 160k     | 0.30 / 0.88                           |\n",
        "| deepseek-ai/DeepSeek-V3                            | 160k     | 0.38 / 0.89                           |\n",
        "| deepseek-ai/DeepSeek-Prover-V2-671B                | 160k     | 0.50 / 2.18                           |\n",
        "| deepseek-ai/DeepSeek-R1-Distill-Llama-70B          | 128k     | 0.10 / 0.40                           |\n",
        "| deepseek-ai/DeepSeek-R1-Distill-Qwen-32B           | 128k     | 0.12 / 0.18                           |\n",
        "| meta-llama/Llama-4-Scout-17B-16E-Instruct                   | 320k     | 0.08 / 0.30                           |\n",
        "| meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8               | 1024k    | 0.15 / 0.60                           |\n",
        "| meta-llama/Llama-4-Maverick-17B-128E-Instruct-Turbo         | 8k       | 0.50 / 0.50                           |\n",
        "| meta-llama/Llama-Guard-4-12B                       | 160k     | 0.05 / 0.05                           |\n",
        "| meta-llama/Llama-3.3-70B-Instruct                  | 128k     | 0.23 / 0.40                           |\n",
        "| meta-llama/Llama-3.3-70B-Instruct-Turbo            | 128k     | 0.07 / 0.25                           |\n",
        "| meta-llama/Llama-3.2-11B-Vision-Instruct           | 128k     | 0.049 / 0.049                         |\n",
        "| meta-llama/Llama-3.2-3B-Instruct                   | 128k     | 0.01 / 0.02                           |\n",
        "| meta-llama/Llama-3.2-1B-Instruct                   | 128k     | 0.005 / 0.01                          |\n",
        "| meta-llama/Meta-Llama-3.1-405B-Instruct            | 32k      | 0.80 / 0.80                           |\n",
        "| meta-llama/Meta-Llama-3.1-70B-Instruct             | 128k     | 0.23 / 0.40                           |\n",
        "| meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo       | 128k     | 0.10 / 0.28                           |\n",
        "| meta-llama/Meta-Llama-3.1-8B-Instruct              | 128k     | 0.03 / 0.05                           |\n",
        "| meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo        | 128k     | 0.019 / 0.03                          |\n",
        "| meta-llama/Meta-Llama-3-70B-Instruct               | 8k       | 0.30 / 0.40                           |\n",
        "| meta-llama/Meta-Llama-3-8B-Instruct                | 8k       | 0.03 / 0.06                           |\n",
        "| Qwen/QwQ-32B                                       | 128k     | 0.15 / 0.20                           |\n",
        "| Qwen/Qwen3-235B-A22B                               | 40k      | 0.13 / 0.60                           |\n",
        "| Qwen/Qwen3-32B                                     | 40k      | 0.10 / 0.30                           |\n",
        "| Qwen/Qwen3-30B-A3B                                 | 40k      | 0.08 / 0.29                           |\n",
        "| Qwen/Qwen3-14B                                     | 40k      | 0.06 / 0.24                           |\n",
        "| Qwen/Qwen2.5-72B-Instruct                          | 32k      | 0.12 / 0.39                           |\n",
        "| Qwen/Qwen2.5-Coder-32B-Instruct                    | 32k      | 0.06 / 0.15                           |\n",
        "| Qwen/Qwen2.5-7B-Instruct                           | 32k      | 0.04 / 0.10                           |\n",
        "| mistralai/Mixtral-8x7B-Instruct-v0.1               | 2k       | 0.08 / 0.24                           |\n",
        "| microsoft/WizardLM-2-8x22B                          | 64k      | 0.48 / 0.48                           |\n",
        "| mistralai/Mistral-7B-Instruct-v0.3                  | 2k       | 0.028 / 0.054                         |\n",
        "| Gryphe/MythoMax-L2-13b                           | 4k       | 0.065 / 0.065                         |\n",
        "                       |"
      ],
      "metadata": {
        "id": "r01hind_f4i-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prompt Zero Shot"
      ],
      "metadata": {
        "id": "_qsaAqhmj7D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatDeepInfra\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "chat = ChatDeepInfra(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")  #Escolhendo o modelo mais barato =)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Voc√™ √© um assistente que responde com ironia\"), #Definindo a personalidade do seu agente.\n",
        "    HumanMessage(content=\"Qual a capital do Brasil?\")\n",
        "]\n",
        "\n",
        "resposta = chat.invoke(messages)\n",
        "print(resposta.content)  # Imprimindo apenas a mensagem de resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqq12ezqH72L",
        "outputId": "c0359024-cd49-4163-8fb1-f22c7c3db28c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duvidoso! Voc√™ n√£o sabe que a capital do Brasil √© Bras√≠lia? N√£o √© a mulher da sua vida... ou talvez seja, os brasileiros n√£o s√£o conhecidos por seu conhecimento sobre hist√≥ria pol√≠tica! Bras√≠lia, sim! (rindo)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(resposta.response_metadata) # Imprimindo os metadados da resposta\n",
        "print(f'Tipo da resposta? {type(resposta)}') #Perceba que resposta √© uma AIMessage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyitO2I3jdpi",
        "outputId": "383df793-6c31-4806-9a82-bc7c908fbcf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'token_usage': {'prompt_tokens': 32, 'total_tokens': 85, 'completion_tokens': 53, 'estimated_cost': 1.54e-06}, 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', 'finish_reason': 'stop'}\n",
            "Tipo da resposta? <class 'langchain_core.messages.ai.AIMessage'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zB6dOKiIe3Ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prompt Few Shot"
      ],
      "metadata": {
        "id": "Ky6vkgiAmW_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatDeepInfra(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")  #Escolhendo o modelo mais barato =)\n",
        "\n",
        "messages = [\n",
        "    HumanMessage(content=\"Qual o segundo dia da semana?\"),\n",
        "    AIMessage(content=\"Segunda\"),\n",
        "    HumanMessage(content=\"Qual o terceiro dia da semana?\"),\n",
        "    AIMessage(content=\"Ter√ßa\"),\n",
        "    HumanMessage(content=\"Qual o √∫ltimo dia da semana?\")\n",
        "]\n",
        "\n",
        "resposta = chat.invoke(messages)\n",
        "print(resposta.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm7Tmu_XlKYD",
        "outputId": "071d41b4-f2c1-468c-d0ce-6e3e1e7c5cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S√°bado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Armazenar respostas em cache\n",
        "\n",
        "O LangChain fornece uma camada de cache opcional para chat models. Isso √© √∫til por dois motivos principais:\n",
        "\n",
        "1.   Isso pode economizar dinheiro reduzindo o n√∫mero de chamadas de API que voc√™ faz ao provedor de LLM, caso voc√™ frequentemente solicite a mesma conclus√£o v√°rias vezes. Isso √© especialmente √∫til durante o desenvolvimento de aplicativos.\n",
        "2.   Ele pode acelerar seu aplicativo reduzindo o n√∫mero de chamadas de API que voc√™ faz ao provedor de LLM.\n",
        "\n"
      ],
      "metadata": {
        "id": "X2tGF_2wot7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.cache import InMemoryCache\n",
        "from langchain.globals import set_llm_cache\n",
        "\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "chat = ChatDeepInfra(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")  #Escolhendo o modelo mais barato =)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Voc√™ √© um assistente que responde com ironia\"), #Definindo a personalidade do seu agente.\n",
        "    HumanMessage(content=\"Qual o primeiro dia da semana?\")\n",
        "]"
      ],
      "metadata": {
        "id": "AmyHbhBKpTFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Observe que o tempo de resposta (Wall time) na 1¬™ chamada √© bem superior ao da 2¬™ chamada, pois a resposta j√° est√° em cache.\n",
        "%%time\n",
        "chat.invoke(messages).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "O0f_wx4FqTAm",
        "outputId": "dd9ffd07-169c-42e8-9e1f-1bae403ad4d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.2 ms, sys: 0 ns, total: 13.2 ms\n",
            "Wall time: 629 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sim, √© o √∫nico dia que realmente define o resto da semana... eu me refiro, √© o domingo. N√£o tenha medo, n√£o vou perguntar se voc√™ vai acordar cedo para essa inc√≥gnita importante.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "chat.invoke(messages).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "qjGtHvjZqYNV",
        "outputId": "7fcc5caa-dbfc-42b3-eb4e-4d12dbe804b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 825 ¬µs, sys: 0 ns, total: 825 ¬µs\n",
            "Wall time: 815 ¬µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sim, √© o √∫nico dia que realmente define o resto da semana... eu me refiro, √© o domingo. N√£o tenha medo, n√£o vou perguntar se voc√™ vai acordar cedo para essa inc√≥gnita importante.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prompt Templates\n",
        "\n",
        "Prompt Templates ajudam a traduzir a entrada e os par√¢metros do usu√°rio em instru√ß√µes para um modelo de linguagem. Isso pode ser usado para orientar a resposta de um modelo, ajudando-o a entender o contexto e gerar resultados relevantes e coerentes baseados na linguagem."
      ],
      "metadata": {
        "id": "1nVW7ThHrzSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Voc√™ √© um assistente chamado {nome_assistente} e sempre responde com no m√°ximo {n_palavras} palavras\"),\n",
        "    (\"human\", \"Ol√°, como vai?\"),\n",
        "    (\"ai\", \"Estou bem, como posso lhe ajudar?\"),\n",
        "    (\"human\", \"{pergunta}\")\n",
        "  ]\n",
        ")\n",
        "\n",
        "partial_chat_template = chat_template.partial(nome_assistente=\"BotX\") #Para definir alguma vari√°vel do prompt template como default\n",
        "\n",
        "partial_chat_template.format_messages(nome_assistente=\"BotX\", n_palavras=\"10\", pergunta=\"Qual o seu nome?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkZk49yVtphd",
        "outputId": "6907f1ad-2285-43ff-fbc2-f43b1cf9fd50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Voc√™ √© um assistente chamado BotX e sempre responde com no m√°ximo 10 palavras', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Ol√°, como vai?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Estou bem, como posso lhe ajudar?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Qual o seu nome?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatDeepInfra(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")  #Escolhendo o modelo mais barato =)\n",
        "\n",
        "chat.invoke(partial_chat_template.format_messages(nome_assistente=\"Future\", n_palavras=\"1\", pergunta=\"Qual seu nome?\")).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "D4jk0H6aw30d",
        "outputId": "7bd2211b-624b-41a5-83d3-688473f399f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Future.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Few Shot com Prompt Template"
      ],
      "metadata": {
        "id": "aPMewa0jyOl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exemplos = [\n",
        "   {\"pergunta\": \"Qual √© a maior montanha do mundo, o Monte Everest ou o K2?\",\n",
        "     \"resposta\":\n",
        "     \"\"\"S√£o necess√°rias perguntas de acompanhamento aqui: Sim.\n",
        "        Pergunta de acompanhamento: Qual √© a altura do Monte Everest?\n",
        "        Resposta intermedi√°ria: O Monte Everest tem 8.848 metros de altura.\n",
        "        Pergunta de acompanhamento: Qual √© a altura do K2?\n",
        "        Resposta intermedi√°ria: O K2 tem 8.611 metros de altura.\n",
        "        Ent√£o a resposta final √©: Monte Everest\n",
        "    \"\"\",\n",
        "    },\n",
        "    {\"pergunta\": \"Quem nasceu primeiro, Charles Darwin ou Albert Einstein?\",\n",
        "     \"resposta\":\n",
        "     \"\"\"S√£o necess√°rias perguntas de acompanhamento aqui: Sim.\n",
        "      Pergunta de acompanhamento: Quando nasceu Charles Darwin?\n",
        "      Resposta intermedi√°ria: Charles Darwin nasceu em 12 de fevereiro de 1809.\n",
        "      Pergunta de acompanhamento: Quando nasceu Albert Einstein?\n",
        "      Resposta intermedi√°ria: Albert Einstein nasceu em 14 de mar√ßo de 1879.\n",
        "      Ent√£o a resposta final √©: Charles Darwin\n",
        "      \"\"\",\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "soiJ42O6yToF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"human\", \"{pergunta}\"),\n",
        "     (\"ai\", \"{resposta}\")]\n",
        ")\n",
        "\n",
        "few_shot_template = FewShotChatMessagePromptTemplate(\n",
        "    examples=exemplos,\n",
        "    example_prompt=example_prompt\n",
        ")\n",
        "\n",
        "prompt_final = ChatPromptTemplate.from_messages([\n",
        "    few_shot_template,\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "prompt = prompt_final.format_messages(input=\"Quem fez mais gols, Messi ou Cristiano Ronaldo?\")\n",
        "chat.invoke(prompt).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LTzcbQz545cu",
        "outputId": "b7cec38a-6f9a-486c-cb4a-5098a5222d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'S√£o necess√°rias perguntas de acompanhamento aqui: Sim.\\n      Pergunta de acompanhamento: Quantos gols marcaram Messi em sua carreira?\\n      Resposta intermedi√°ria: Messi marcou 772 gols em sua carreira.\\n      Pergunta de acompanhamento: Quantos gols marcaram Cristiano Ronaldo em sua carreira?\\n      Resposta intermedi√°ria: Ronaldo marcou 819 gols em sua carreira.\\n      Ent√£o a resposta final √©: Cristiano Ronaldo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Output Parsers\n",
        "\n",
        "Output Parsers s√£o respons√°veis ‚Äã‚Äãpor pegar a sa√≠da de um LLM e transform√°-la em um formato mais adequado. Isso √© muito √∫til quando voc√™ usa LLMs para gerar qualquer tipo de dado estruturado.\n",
        "\n",
        "No exemplo abaixo vamos pegar um texto que simula a avalia√ß√£o de um cliente sobre um produto e extrair algumas informa√ß√µes em formato JSON. Vamos extrair o nome do produto, as caracter√≠sticas avaliadas como positivas, as caracter√≠sticas avaliadas como negativas e se o cliente recomenda ou n√£o o produto. Por fim, conseguiremos acessar essas informa√ß√µes em JSON."
      ],
      "metadata": {
        "id": "di9pgKlwZAL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback_produto = \"\"\"\n",
        "Estou muito satisfeito com o Smartphone XYZ Pro. O desempenho √© excelente, e o sistema operacional √© r√°pido e intuitivo. A c√¢mera √© um dos principais destaques, especialmente o\n",
        "modo noturno, que captura imagens incr√≠veis mesmo em baixa ilumina√ß√£o. A dura√ß√£o da bateria tamb√©m impressiona, durando facilmente um dia inteiro com uso intenso.\n",
        "Por outro lado, sinto que o produto poderia ser melhorado em alguns aspectos. A tela, embora tenha cores vibrantes, parece refletir bastante luz, dificultando o uso sob o sol.\n",
        "Al√©m disso, o carregador inclu√≠do na caixa n√£o oferece carregamento r√°pido, o que √© um ponto negativo considerando o pre√ßo do aparelho.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CwuwwhELZQ2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "\n",
        "schema_produto = ResponseSchema(\n",
        "    name=\"produto\",\n",
        "    type=\"string\",\n",
        "    description=\"Nome do produto mencionado no texto\"\n",
        ")\n",
        "\n",
        "schema_positivas = ResponseSchema(\n",
        "    name=\"caracter√≠sticas_positivas\",\n",
        "    type=\"list\",\n",
        "    description=\"Liste todas as caracter√≠sticas positivas mencionadas sobre o produto\"\n",
        ")\n",
        "\n",
        "schema_negativas = ResponseSchema(\n",
        "    name='caracter√≠sticas_negativas',\n",
        "    type='list',\n",
        "    description='Liste todas as caracter√≠sticas negativas mencionadas sobre o produto.'\n",
        ")\n",
        "\n",
        "schema_recomendacao = ResponseSchema(\n",
        "    name='recomenda√ß√£o',\n",
        "    type='bool',\n",
        "    description='O cliente recomenda o produto? Responda True para sim ou False para n√£o.'\n",
        ")"
      ],
      "metadata": {
        "id": "jQGPwhqDmaJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "response_schema = [schema_produto, schema_positivas, schema_negativas, schema_recomendacao]\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
        "schema_formatado = output_parser.get_format_instructions()\n",
        "\n",
        "review_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "Para o texto a seguir, extraia as seguintes informa√ß√µes:\n",
        "produto, caracter√≠sticas_positivas, caracter√≠sticas_negativas e recomendacao\n",
        "\n",
        "Texto: {review}\n",
        "\n",
        "{schema}\n",
        "\"\"\", partial_variables={\"schema\":schema_formatado}\n",
        ")\n",
        "\n",
        "\n",
        "resposta = chat.invoke(review_template.format_messages(review=feedback_produto))\n",
        "\n",
        "resposta_json = output_parser.parse(resposta.content)\n",
        "\n",
        "resposta_json"
      ],
      "metadata": {
        "id": "aLyRRZAHhlb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d310b92-e47f-4fd2-e1b8-2cdc4ec34722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'produto': 'Smartphone XYZ Pro',\n",
              " 'caracter√≠sticas_positivas': ['Desempenho excelente',\n",
              "  'Sistema operacional r√°pido e intuitivo',\n",
              "  'C√¢mera de alta qualidade, especialmente no modo noturno',\n",
              "  'Dura√ß√£o da bateria de at√© um dia inteiro com uso intenso',\n",
              "  'Tela com cores vibrantes'],\n",
              " 'caracter√≠sticas_negativas': ['A tela reflete bastante luz, dificultando o uso sob o sol',\n",
              "  'Carregador inclu√≠do na caixa n√£o oferece carregamento r√°pido'],\n",
              " 'recomenda√ß√£o': True}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta_json[\"produto\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "z3DPmUljiAxv",
        "outputId": "927e757e-a653-4876-9b04-dd53498da4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Smartphone XYZ Pro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta_json[\"caracter√≠sticas_positivas\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iurERZUhh-7z",
        "outputId": "2ceb5de5-d2f6-44e4-de06-fca992e8681a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Desempenho excelente',\n",
              " 'Sistema operacional r√°pido e intuitivo',\n",
              " 'C√¢mera de alta qualidade, especialmente no modo noturno',\n",
              " 'Dura√ß√£o da bateria de at√© um dia inteiro com uso intenso',\n",
              " 'Tela com cores vibrantes']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta_json[\"caracter√≠sticas_negativas\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa7HaYFwh1xc",
        "outputId": "1f3eb567-d3cd-4123-e8b9-2ec71d387c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A tela reflete bastante luz, dificultando o uso sob o sol',\n",
              " 'Carregador inclu√≠do na caixa n√£o oferece carregamento r√°pido']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta_json[\"recomenda√ß√£o\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBUKrkW8hvWl",
        "outputId": "5d55245d-3305-4c58-fc25-fd7405683d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}